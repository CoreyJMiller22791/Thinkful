{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "# Try to use code from THINKFUL examples to scrape some data from amazon.\n",
    "import scrapy\n",
    "import re\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "class AmazonScraper(scrapy.Spider):\n",
    "    # Naming the spider is important if you are running more than one spider of\n",
    "    # this class simultaneously.\n",
    "    name = \"AmazonScraper\"\n",
    "    \n",
    "    # URL(s) to start with.\n",
    "    start_urls = [\n",
    "        'https://www.amazon.com/s/ref=nb_sb_noss?url=search-alias%3Dgarden&field-keywords=blender',\n",
    "    ]\n",
    "\n",
    "    # Use XPath to parse the response we get.\n",
    "    def parse(self, response):\n",
    "        \n",
    "        # Iterate over every <li> element on the page.\n",
    "        # Each li contains the product container for each result.\n",
    "        # // is a shortcut for finding all li regardless of parent level\n",
    "        for article in response.xpath('//li'):\n",
    "            \n",
    "            # Yield a dictionary with the values we want.\n",
    "            # attempting to get the top results for blenders - \n",
    "            # we want blender name, result id, rank, ASIN, and would like link\n",
    "            yield {\n",
    "                'name': article.xpath('a/h2/@data-attribute').extract_first(),\n",
    "                'result_id': article.xpath('@id').extract_first(),\n",
    "                'ASIN': article.xpath('@data-asin').extract_first(),\n",
    "                'rank': article.xpath('@data-result-rank').extract_first()\n",
    "            }\n",
    "        # Get the URL of the previous page.\n",
    "        #next_page = response.xpath('//div[@class=\"nav-previous\"]/a/@href').extract_first()\n",
    "        \n",
    "        # There are a LOT of pages here.  For our example, we'll just scrape the first 9.\n",
    "        # This finds the page number. The next segment of code prevents us from going beyond page 9.\n",
    "        #pagenum = int(re.findall(r'\\d+',next_page)[0])\n",
    "        \n",
    "        # Recursively call the spider to run on the next page, if it exists.\n",
    "        #if next_page is not None and pagenum < 10:\n",
    "            #next_page = response.urljoin(next_page)\n",
    "            # Request the next page and recursively parse it the same way we did above\n",
    "            #yield scrapy.Request(next_page, callback=self.parse)\n",
    "\n",
    "# Tell the script how to run the crawler by passing in settings.\n",
    "# The new settings have to do with scraping etiquette.          \n",
    "process = CrawlerProcess({\n",
    "    'FEED_FORMAT': 'json',          # Store data in JSON format.\n",
    "    'FEED_URI': 'blender_amazon.json',       # Name our storage file. Doesn't resave over old file? - \n",
    "                                             # have to delete and rerun to save new\n",
    "    'LOG_ENABLED': False,          # Turn off logging for now.\n",
    "    'ROBOTSTXT_OBEY': True,\n",
    "    'AUTOTHROTTLE_ENABLED': True,\n",
    "    'HTTPCACHE_ENABLED': True,\n",
    "    'USER-AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.110 Safari/537.36'\n",
    "})\n",
    "\n",
    "# Start the crawler with our spider.\n",
    "process.crawl(AmazonScraper)\n",
    "process.start()\n",
    "print('Success!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(186, 4)\n",
      "          ASIN  name  rank                         result_id\n",
      "0         None   NaN   NaN                              None\n",
      "1   B00NGV4506   NaN   0.0                          result_0\n",
      "2   B00EI7DPI0   NaN   1.0                          result_1\n",
      "3   B007TIE0GQ   NaN   2.0                          result_2\n",
      "4   B017TZ9SME   NaN   3.0                          result_3\n",
      "5   B012T634SM   NaN   4.0                          result_4\n",
      "6   B0019MLLCO   NaN   5.0                          result_5\n",
      "7   B00939FV8K   NaN   6.0                          result_6\n",
      "8   B07CX95VRT   NaN   7.0                          result_7\n",
      "9   B07BS3H5VF   NaN   8.0                          result_8\n",
      "10  B0081PTLGU   NaN   9.0                          result_9\n",
      "11  B008H4SLV6   NaN  10.0                         result_10\n",
      "12  B00Y2U1QUM   NaN  11.0                         result_11\n",
      "13  B0764BD7WV   NaN  12.0                         result_12\n",
      "14  B000GIGZXM   NaN  13.0                         result_13\n",
      "15  B00DBQ1AIG   NaN  14.0                         result_14\n",
      "16  B00JR5AD6A   NaN  15.0                         result_15\n",
      "17  B004P2OLB8   NaN  16.0                         result_16\n",
      "18  B075X1KPLZ   NaN  17.0                         result_17\n",
      "19  B003XU3C7M   NaN  18.0                         result_18\n",
      "20  B00939I7EK   NaN  19.0                         result_19\n",
      "21  B00CGSES9I   NaN  20.0                         result_20\n",
      "22  B0758JHZM3   NaN  21.0                         result_21\n",
      "23        None   NaN   NaN                              None\n",
      "24        None   NaN   NaN  s-result-list-layout-placeholder\n",
      "25        None   NaN   NaN                              None\n",
      "26        None   NaN   NaN                              None\n",
      "27        None   NaN   NaN                              None\n",
      "28        None   NaN   NaN                              None\n",
      "29        None   NaN   NaN                              None\n",
      "30        None   NaN   NaN                              None\n",
      "     ASIN  name  rank result_id\n",
      "181  None   NaN   NaN      None\n",
      "182  None   NaN   NaN      None\n",
      "183  None   NaN   NaN      None\n",
      "184  None   NaN   NaN      None\n",
      "185  None   NaN   NaN      None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Checking whether we got data\n",
    "Amazondf=pd.read_json('blender_amazon.json', orient='records')\n",
    "print(Amazondf.shape)\n",
    "print(Amazondf.head(31))\n",
    "print(Amazondf.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Should be over 1K results - maybe only got first 184 because only the first 31 blenders (sorted by featured) are shown on the first page.\n",
    "\n",
    "- Also the ASIN doesn't seem to be working correctly.  The 22 result on the amazon page corresponds to the 15th index in the JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
